Non-Contiguous Memory Allocation is a **memory management technique where a process is allowed to occupy different sections of physical memory that are not necessarily contiguous**. This approach helps in better memory utilization and **eliminates external fragmentation**. It **breaks the process into smaller chunks and allocates them** to available memory spaces without requiring the entire process to be stored in one continuous block of memory.


### Benefits of Non-Contiguous Memory Allocation:
1. **No External Fragmentation**:
    - Since memory can be allocated in non-contiguous blocks, there are no large unusable memory gaps (external fragmentation).  
2. **Efficient Memory Utilization**:
    - Free memory can be used more efficiently by allocating smaller chunks to different parts of the process, allowing better use of the available memory.  
3. **Scalability**:
    - Processes can grow dynamically in size by acquiring additional pages or segments without needing to relocate the entire process in memory.

### Drawbacks:
1. **Overhead in Address Translation**:
    - Non-contiguous memory allocation requires complex address translation mechanisms, such as page tables or segment tables, adding overhead to the system.
2. **Internal Fragmentation**:
    - In paging, if the last page of a process is not fully utilized, internal fragmentation occurs within the page.

---
### Common Techniques:
- **Paging**:
    - Divides the memory into fixed-size pages and frames, allowing each process to be allocated frames that need not be contiguous in physical memory.
      (More detailed explanation below)
- **Segmentation**:
    - Divides the memory into variable-sized segments based on the logical division of a program, which are allocated in non-contiguous parts of memory.
      (More detailed explanation below)



## 1. Paging


#### Need for Paging

**Disadvantage of Dynamic Partitioning**: The main disadvantage of Dynamic Partitioning is External fragmentation. Although, this can be removed by Compaction but as we have discussed earlier, the compaction makes the system inefficient. We need to find out a mechanism which can load the processes in the partitions in a more optimal way. Let us discuss a dynamic and flexible mechanism called paging.


Lets consider a process P1 of size 2 MB and the main memory which is divided into three partitions. Out of the three partitions, two partitions are holes of size 1 MB each.

P1 needs 2 MB space in the main memory to be loaded. We have two holes of 1 MB each but they are not contiguous.

Although, there is 2 MB space available in the main memory in the form of those holes but that remains useless until it become contiguous. This is a serious problem to address.

We need to have some kind of mechanism which can store one process at different locations of the memory.

**The Idea behind paging is to divide the process in pages** so that, we can store them in the memory at different holes.

  ![[os-need-for-paging.png]]



#### Paging
**Paging** is a memory management technique used to manage how processes access and utilize memory. It divides the process's memory space and the physical memory (RAM) into fixed-size blocks called **pages** (in the process) and **frames** (in memory). Each page of the process can be stored in any available frame in the memory, eliminating the need for contiguous memory allocation.

###### Key Concepts in Paging:

1. **Pages and Frames**:
    - **Page**: A fixed-size block of data in a processâ€™s memory space.
    - **Frame**: A fixed-size block of physical memory (RAM) where a page can be loaded. Pages and frames are of the same size to simplify management.
      
2. **Page Table**: Each process has its own **page table** that keeps track of where each page of the process is stored in memory. It maps the page number to the corresponding frame in physical memory. This helps the CPU translate logical addresses (used by the process) into physical addresses (used by RAM).
   Address of the Page Table in the MM is stored in the PCB of each process.
    
3. **Logical Address vs. Physical Address**:
    - **Logical Address**: The address generated by the CPU during a process execution.
    - **Physical Address**: The actual location in the main memory (RAM) where the process's data is stored. The page table helps in converting logical addresses into physical addresses.
      
4. **Paging Mechanism**:
    - When a process is executing, the CPU generates a logical address that needs to be translated to a physical address.
    - The logical address is divided into two parts: **page number** and **page offset**.
    - The **page number** is used to look up the frame number in the page table.
    - The **page offset** is then added to the frame number to get the final physical address.


![[paging-diagram.jpg]]


###### Advantages of Paging
- **No External Fragmentation**: Since pages are of fixed size and can be placed in any available frame, external fragmentation is eliminated.
- **Efficient Use of Memory**: Pages can be stored in any free frame, meaning non-contiguous memory can be utilized effectively.

###### Disadvantages of Paging
 - **Internal Fragmentation**: Since pages are fixed size, if a process does not completely fill a page, the leftover space within the page is wasted.
- **Overhead of Page Table**: Maintaining a page table for each process adds memory overhead, especially for large processes.


##### TLB (Translation Lookaside Buffer)

The **Translation Lookaside Buffer (TLB)** is a special cache in the CPU used to speed up the process of translating **virtual addresses** (logical address) to **physical addresses**. It is an integral part of the **memory management unit (MMU)**, which handles virtual memory in modern operating systems.

###### Purpose of the TLB
Virtual memory allows a process to use addresses that are not tied to the actual physical memory addresses. The **page table** is a data structure that maps these **virtual addresses** to **physical memory addresses**. However, since accessing the page table in memory can be slow, the **TLB** is used to cache the most frequently used page table entries.

###### How the TLB Works

1. **Virtual-to-Physical Address Translation:**
    - When a process accesses a memory address, that address is a **virtual address**.
    - The MMU needs to translate the virtual address into the **physical address** where the data is actually stored.
      
2. **TLB Lookup:**
    - Before checking the **page table** in memory, the MMU first checks the **TLB**.
    - If the **TLB** contains the mapping for the requested virtual address (this is called a **TLB hit**), the physical address is retrieved directly from the TLB.
    - If the **TLB** does **not** contain the mapping (this is called a **TLB miss**), the MMU must look up the virtual-to-physical address mapping in the **page table**.
      
3. **Handling a TLB Miss:**
    - On a **TLB miss**, the page table is consulted, and once the correct address mapping is found, the MMU updates the TLB with this mapping.
    - The next time the same virtual address is accessed, the TLB will contain the mapping, speeding up the process.

1. **TLB Replacement:**
    - The **TLB** has limited storage, so when it is full and a new mapping needs to be added, one of the existing mappings must be replaced.
    - TLB replacement algorithms (like **LRU**, Least Recently Used) are used to decide which entry to evict.


  ![[Translation_Lookaside_Buffer.png]]

###### Why the TLB Is Important
- **Page tables** can be large and stored in main memory, making them slower to access.
- **The TLB acts as a cache**, reducing the number of memory accesses required to translate virtual addresses to physical addresses, thus improving system performance.

###### Example: How TLB Fits in the Memory Access Process
1. **Program requests data** stored at virtual address `VA`.
2. The CPU checks the **TLB**:
    - If the **TLB hit**, the corresponding physical address `PA` is retrieved quickly.
    - If **TLB miss**, the MMU looks up the **page table** to find `PA`:
        - The TLB is updated with this new mapping (`VA` to `PA`).
3. The CPU accesses the data at `PA`.



##### Page table handling during context switching with paging:

1. **Saving the Current Process's Page Table**
When the operating system decides to switch from one process (let's call it **Process A**) to another (let's call it **Process B**), the **page table** for **Process A** needs to be saved:
- The **page table base register** (often called the **CR3** register in x86 architecture) holds the address of the page table for the currently running process.
- During a context switch, the OS saves the **page table base register** (CR3) of **Process A** along with other CPU state information (like the program counter, general registers, etc.) in **Process A's PCB** (Process Control Block).

2. **Loading the Page Table for the Next Process**
After saving the state of **Process A**, the OS prepares to switch to **Process B**:
- The OS retrieves the **page table base register (CR3)** for **Process B** from **Process B's PCB**.
- This new page table base register now points to **Process B's** page table, which maps **Process B's virtual memory** to its corresponding **physical memory** locations.

3. **Flushing the Translation Lookaside Buffer (TLB)**
Once the page table for **Process B** is loaded, the OS typically **flushes the TLB** (Translation Lookaside Buffer). The TLB is a cache used by the CPU to speed up virtual-to-physical address translations:
- Since each process has its own page table, the **TLB entries for Process A** are now invalid, as they belong to **Process A's** virtual memory space.
- **Flushing the TLB** ensures that the next memory accesses made by **Process B** use **Process B's** page table for translation, rather than stale entries from **Process A**.

4. **Memory Access Using the New Page Table**
After the page table switch, all subsequent memory accesses by **Process B** will use its own **page table** to map virtual addresses to physical memory. The CPU, through the MMU (Memory Management Unit), will use the new page table to handle address translations for **Process B**.

###### Problem with Flushing of the TLB
Flushing the TLB (Translation Lookaside Buffer) is inconvenient because it slows down the system. The TLB stores recent translations of virtual memory addresses to physical memory addresses, making memory access much faster. When the TLB is flushed (cleared), the system loses those cached translations, and it has to go through the slower process of looking them up again, causing a delay.

This problem becomes more noticeable during context switches, where the operating system switches between running different processes. Each process has its own memory space, so the TLB entries for one process are not valid for another. Without any way to tell the difference between processes, the system has to flush the entire TLB when switching between them, which means the next process starts with no cached memory addresses, leading to slower memory access until the TLB is refilled.

###### Solution
To solve this, Address Space Identifiers (ASIDs) are used. ASIDs allow the TLB to store memory address translations for different processes at the same time. Each translation in the TLB is tagged with an identifier that matches the process it belongs to. When the operating system switches to a new process, it just updates the ASID instead of flushing the TLB. This means the TLB can keep useful entries for multiple processes, speeding up memory access without the need to clear everything.


#### Segmentation


**What is Segmentation?**

- Segmentation is a memory management method where memory is divided into **variable-sized parts** called segments. Each segment is allocated to a process.
- The details of each segment are stored in a **segment table**, which is also stored in memory.
- The segment table contains two key pieces of information for each segment:
    - **Base**: Starting address of the segment.
    - **Limit**: Size of the segment.
  
  ![[os-segmentation.png]]
---

**Why is Segmentation Needed?**

- **Paging** divides memory into fixed-size pages, which is better suited for the operating system but not ideal for the userâ€™s view of the program.
- Operating system doesn't care about the User's view of the process. It may divide the same function into different pages and those pages may or may not be loaded at the same time into the memory. It decreases the efficiency of the system.
- **Segmentation** keeps related parts of a program (like the main function or library functions) together in the same segment, making it more user-friendly and efficient.
  ![[os-segmentation-segments.png]]

---

**Logical to Physical Address Translation:**

- The CPU generates a **logical address** with two parts:
    - **Segment Number**: Identifies the segment.
    - **Offset**: Points to the specific location within the segment.

Example:  
In a 16-bit address, 4 bits might be for the segment number and 12 bits for the offset. This means a program can have **up to 16 segments**, each with a maximum size of **4096 units**.
  
---

**How Segmentation Works:**

1. When a program is loaded into memory, the operating system finds free space for each segment from the **free list** maintained by the memory manager.
2. Each segment is loaded into memory, and a **segment map table** is generated for the program.
3. The **segment number** is mapped to the segment table, and the **limit** of the respective segment is compared with the offset. If the offset is less than the limit then the address is valid otherwise it throws an error as the address is invalid.
4. If the offset is valid, the **base address** of the segment is added to the offset to calculate the **physical address**.

   ![[segmentation3.webp]]

---

**Advantages of Segmentation:**

- No **internal fragmentation** (no wasted space within a segment).
- **Larger segment size** compared to fixed page sizes.
- **Less overhead** than paging.
- Segments are easier to relocate than moving the entire address space.
- Segment tables are **smaller** than page tables in paging.

---

**Disadvantages of Segmentation:**

- Can suffer from **external fragmentation** (holes between segments).
- Allocating contiguous memory for variable-sized segments can be challenging.
- Memory management for segments requires **complex algorithms**.